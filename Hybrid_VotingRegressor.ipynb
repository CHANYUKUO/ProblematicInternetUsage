{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1db04ef8",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-12-04T04:57:45.777705Z",
     "iopub.status.busy": "2024-12-04T04:57:45.777366Z",
     "iopub.status.idle": "2024-12-04T04:57:48.546749Z",
     "shell.execute_reply": "2024-12-04T04:57:48.545780Z"
    },
    "papermill": {
     "duration": 2.775939,
     "end_time": "2024-12-04T04:57:48.549084",
     "exception": false,
     "start_time": "2024-12-04T04:57:45.773145",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The train data has the shape:  (3960, 81)\n",
      "The test data has the shape:  (20, 58)\n",
      "\n",
      "Total number of missing training values:  131717\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_23/2326840449.py:18: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  train[season] = train[season].replace({'Spring':1, 'Summer':2, 'Fall':3, 'Winter':4})\n",
      "/tmp/ipykernel_23/2326840449.py:25: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  test[season] = test[season].replace({'Spring':1, 'Summer':2, 'Fall':3, 'Winter':4})\n"
     ]
    }
   ],
   "source": [
    "import numpy as np, pandas as pd, os\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "import xgboost as xgb\n",
    "import plotly.express as px, seaborn as sns, matplotlib.pyplot as plt\n",
    "sns.set_style('darkgrid')\n",
    "from sklearn.metrics import make_scorer, cohen_kappa_score\n",
    "path = '../input/child-mind-institute-problematic-internet-use/'\n",
    "sample = pd.read_csv('/kaggle/input/child-mind-institute-problematic-internet-use/sample_submission.csv')\n",
    "train = pd.read_csv(path + 'train.csv', index_col = 'id')\n",
    "print(\"The train data has the shape: \",train.shape)\n",
    "test = pd.read_csv(path + 'test.csv', index_col = 'id')\n",
    "print(\"The test data has the shape: \",test.shape)\n",
    "print(\"\")  \n",
    "print(\"Total number of missing training values: \", train.isna().sum().sum())\n",
    "train_cat_columns = train.select_dtypes(exclude = 'number').columns\n",
    "\n",
    "for season in train_cat_columns:\n",
    "    train[season] = train[season].replace({'Spring':1, 'Summer':2, 'Fall':3, 'Winter':4})\n",
    "PCIAT_cols = [val for val in train.columns[train.columns.str.contains('PCIAT')]]\n",
    "PCIAT_cols.remove('PCIAT-PCIAT_Total')\n",
    "train = train.drop(columns = PCIAT_cols)\n",
    "train = train.dropna(subset='sii')\n",
    "test_cat_columns = test.select_dtypes(exclude = 'number').columns\n",
    "for season in test_cat_columns:\n",
    "    test[season] = test[season].replace({'Spring':1, 'Summer':2, 'Fall':3, 'Winter':4})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dccf1cc5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-04T04:57:48.555751Z",
     "iopub.status.busy": "2024-12-04T04:57:48.555495Z",
     "iopub.status.idle": "2024-12-04T04:57:51.663027Z",
     "shell.execute_reply": "2024-12-04T04:57:51.662068Z"
    },
    "papermill": {
     "duration": 3.112842,
     "end_time": "2024-12-04T04:57:51.664987",
     "exception": false,
     "start_time": "2024-12-04T04:57:48.552145",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Define a custom dataset class that handles sample IDs from the index\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data, targets):\n",
    "        df = pd.DataFrame(data)\n",
    "        df = df.fillna(0) # use median for ratio, 0 for regular\n",
    "        self.data = torch.tensor(df.values, dtype=torch.float32)\n",
    "        self.targets = torch.tensor(targets, dtype=torch.float32)\n",
    "        self.sample_ids = data.index  # Use the index as sample IDs\n",
    "        self.mask=np.array(data.isna().astype(int))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.targets[idx], self.sample_ids[idx],self.mask[idx]\n",
    "\n",
    "# Simple Neural Network Model\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(input_size, 64),\n",
    "            nn.BatchNorm1d(64),  # Add batch normalization\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.BatchNorm1d(32),  # Add batch normalization\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 1),\n",
    "            nn.Sigmoid()  # Output between 0 and 1\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x) * 100\n",
    "def train_XG_mask_nns(data, target_df, num_epochs=200,num_models=3, batch_size=64, lr=0.0025):\n",
    "    models={}\n",
    "    losses=np.array([])\n",
    "    \n",
    "\n",
    "    # Loss function (assuming regression task, modify for classification)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    # Store predictions and sample IDs for mapping later\n",
    "    all_predictions = {}\n",
    "    # Extract targets matching the sample IDs in separated_data\n",
    "    matching_targets = target_df.loc[data.index]  # Ensure target_df index aligns with data.index\n",
    "    \n",
    "    # Create dataset and data loader\n",
    "    dataset = CustomDataset(data, matching_targets.values)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # Initialize model, optimizer, and store them\n",
    "    for ith_model in range(num_models):\n",
    "        print(f\"\\nTraining Model {ith_model}...\")\n",
    "        input_size = data.shape[1]\n",
    "        model = SimpleNN(input_size)\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "        if not losses.size== 0:\n",
    "            # Create a new dataset using the previous losses as targets\n",
    "            dataset = CustomDataset(data, losses)\n",
    "            dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "            losses=np.array([])\n",
    "    # Training loop\n",
    "        for epoch in range(num_epochs):\n",
    "            model.train()\n",
    "            for inputs, target, ids,mask in dataloader:\n",
    "                optimizer.zero_grad()\n",
    "                output = model(inputs).squeeze()\n",
    "                loss = criterion(output, target)\n",
    "                loss= loss * (1-mask).float().mean(dim=1)   # Mask out NaN positions\n",
    "                loss=loss.mean()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            #if epoch % 50==0:\n",
    "                #print(ith_model)\n",
    "                #print('Epoch'+str(epoch))\n",
    "                #print(loss)\n",
    "\n",
    "        model.eval()\n",
    "        for inputs, target, ids,mask in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            output = model(inputs).squeeze()\n",
    "            loss = target-output\n",
    "            loss= loss * (1-mask).float().mean(dim=1)  # Mask out NaN positions\n",
    "            losses=np.append(losses,loss.detach().numpy())\n",
    "\n",
    "        models[ith_model] = model\n",
    "        \n",
    " \n",
    "\n",
    "    return models\n",
    "def run_XG_mask_nns(test_data, models,batch_size=32):\n",
    "    output_list=np.array([])\n",
    "    index_name=[]\n",
    "\n",
    "    # Initialize model, optimizer, and store them\n",
    "    for ith_model in range(len(models)):\n",
    "        model=models[ith_model]\n",
    "        model.eval()\n",
    "        #print(f\"\\nrunning Model...\")\n",
    "        test_inputs = torch.tensor(test_data.values, dtype=torch.float32)\n",
    "        test_inputs = torch.nan_to_num(test_inputs, nan=0.0)\n",
    "    # Training loop\n",
    "        with torch.no_grad():\n",
    "            output = model(test_inputs).squeeze()\n",
    "        if output_list.size==0:\n",
    "            output_list=output\n",
    "        else:\n",
    "            output_list=output_list+output\n",
    "    all_predictions = pd.DataFrame({'Sample_ID':  test_data.index, 'Prediction': output_list})\n",
    "        \n",
    " \n",
    "\n",
    "    return all_predictions\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5960207d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-04T04:57:51.671752Z",
     "iopub.status.busy": "2024-12-04T04:57:51.671004Z",
     "iopub.status.idle": "2024-12-04T04:58:05.475980Z",
     "shell.execute_reply": "2024-12-04T04:58:05.475016Z"
    },
    "papermill": {
     "duration": 13.810304,
     "end_time": "2024-12-04T04:58:05.478007",
     "exception": false,
     "start_time": "2024-12-04T04:57:51.667703",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "from sklearn.base import clone\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from scipy.optimize import minimize\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from tqdm import tqdm\n",
    "import polars as pl\n",
    "import polars.selectors as cs\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator, FormatStrFormatter, PercentFormatter\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense\n",
    "from keras.optimizers import Adam\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from colorama import Fore, Style\n",
    "from IPython.display import clear_output\n",
    "import warnings\n",
    "from lightgbm import LGBMRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.ensemble import VotingRegressor, RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.options.display.max_columns = None\n",
    "from sklearn.base import BaseEstimator, RegressorMixin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2c97db27",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-04T04:58:05.485441Z",
     "iopub.status.busy": "2024-12-04T04:58:05.484248Z",
     "iopub.status.idle": "2024-12-04T04:58:05.491903Z",
     "shell.execute_reply": "2024-12-04T04:58:05.491091Z"
    },
    "papermill": {
     "duration": 0.012743,
     "end_time": "2024-12-04T04:58:05.493603",
     "exception": false,
     "start_time": "2024-12-04T04:58:05.480860",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def convert(scores):\n",
    "    scores = np.array(scores)*1.3\n",
    "    bins = np.zeros_like(scores)\n",
    "    bins[scores <= 30] = 0\n",
    "    bins[(scores > 30) & (scores < 50)] = 1\n",
    "    bins[(scores >= 50) & (scores < 80)] = 2\n",
    "    bins[scores >= 80] = 3\n",
    "    return bins\n",
    "class TrainXGMaskNNSRegressor(BaseEstimator, RegressorMixin):\n",
    "    def __init__(self, num_epochs=200, num_models=3, batch_size=64, lr=0.0025):\n",
    "        self.num_epochs = num_epochs\n",
    "        self.num_models = num_models\n",
    "        self.batch_size = batch_size\n",
    "        self.lr = lr\n",
    "        self.models = None  # This will store trained models after fitting\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # Ensure input data is in DataFrame format for compatibility\n",
    "        data = pd.DataFrame(X)\n",
    "        target_df = pd.Series(y, index=data.index)\n",
    "        \n",
    "        # Train models using the original train_XG_mask_nns function\n",
    "        self.models = train_XG_mask_nns(data, target_df, num_epochs=self.num_epochs, \n",
    "                                        num_models=self.num_models, batch_size=self.batch_size, lr=self.lr)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Convert test data to DataFrame if not already\n",
    "        test_data = pd.DataFrame(X)\n",
    "        \n",
    "        # Use the run_XG_mask_nns function for prediction, which averages model outputs\n",
    "        predictions_df = run_XG_mask_nns(test_data, self.models, batch_size=self.batch_size)\n",
    "        \n",
    "        # Extract and return predictions as a NumPy array\n",
    "        return predictions_df['Prediction'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e802ad42",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-04T04:58:05.499485Z",
     "iopub.status.busy": "2024-12-04T04:58:05.499110Z",
     "iopub.status.idle": "2024-12-04T04:58:46.242142Z",
     "shell.execute_reply": "2024-12-04T04:58:46.240915Z"
    },
    "papermill": {
     "duration": 40.748311,
     "end_time": "2024-12-04T04:58:46.244310",
     "exception": false,
     "start_time": "2024-12-04T04:58:05.495999",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip -q install /kaggle/input/pytorchtabnet/pytorch_tabnet-4.1.0-py3-none-any.whl\n",
    "SEED=42\n",
    "n_splits = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ce39edc9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-04T04:58:46.251444Z",
     "iopub.status.busy": "2024-12-04T04:58:46.251099Z",
     "iopub.status.idle": "2024-12-04T04:58:46.378214Z",
     "shell.execute_reply": "2024-12-04T04:58:46.377304Z"
    },
    "papermill": {
     "duration": 0.133348,
     "end_time": "2024-12-04T04:58:46.380391",
     "exception": false,
     "start_time": "2024-12-04T04:58:46.247043",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pytorch_tabnet.tab_model import TabNetRegressor\n",
    "import torch\n",
    "\n",
    "\n",
    "def quadratic_weighted_kappa(y_true, y_pred):\n",
    "    return cohen_kappa_score(y_true, y_pred, weights='quadratic')\n",
    "\n",
    "def threshold_Rounder(oof_non_rounded, thresholds):\n",
    "    return np.where(oof_non_rounded < thresholds[0], 0,\n",
    "                    np.where(oof_non_rounded < thresholds[1], 1,\n",
    "                             np.where(oof_non_rounded < thresholds[2], 2, 3)))\n",
    "\n",
    "def evaluate_predictions(thresholds, y_true, oof_non_rounded):\n",
    "    rounded_p = threshold_Rounder(oof_non_rounded, thresholds)\n",
    "    return -quadratic_weighted_kappa(y_true, rounded_p)\n",
    "def TrainML(model_class, test_data,classification=True):\n",
    "    X = train.drop(['sii'], axis=1)\n",
    "    X= X.drop(['PCIAT-PCIAT_Total'],axis=1)\n",
    "    y = train['PCIAT-PCIAT_Total']\n",
    "    print(y.dtype)\n",
    "    print(X.shape)\n",
    "    print(y.shape)\n",
    "    print(test_data.shape)\n",
    "    SKF = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=SEED)\n",
    "    \n",
    "    train_S = []\n",
    "    test_S = []\n",
    "    \n",
    "    oof_non_rounded = np.zeros(len(y), dtype=float) \n",
    "    oof_rounded = np.zeros(len(y), dtype=int) \n",
    "    test_preds = np.zeros((len(test_data), n_splits))\n",
    "\n",
    "    for fold, (train_idx, test_idx) in enumerate(tqdm(SKF.split(X, train['sii']), desc=\"Training Folds\", total=n_splits)):\n",
    "        X_train, X_val = X.iloc[train_idx], X.iloc[test_idx]\n",
    "        y_train, y_val = y.iloc[train_idx], y.iloc[test_idx]\n",
    "\n",
    "        model = clone(model_class)\n",
    "        print(model)\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        y_train_pred = model.predict(X_train) \n",
    "        y_val_pred = model.predict(X_val) \n",
    "\n",
    "        oof_non_rounded[test_idx] = y_val_pred\n",
    "        y_val_pred_rounded = y_val_pred.round(0).astype(int)\n",
    "        oof_rounded[test_idx] = y_val_pred_rounded\n",
    "\n",
    "        train_kappa = quadratic_weighted_kappa(convert(y_train), y_train_pred.round(0).astype(int)) \n",
    "        val_kappa = quadratic_weighted_kappa(convert(y_val), y_val_pred_rounded) \n",
    "\n",
    "        train_S.append(train_kappa)\n",
    "        test_S.append(val_kappa)\n",
    "        \n",
    "        test_preds[:, fold] = model.predict(test_data) \n",
    "        \n",
    "        print(f\"Fold {fold+1} - Train QWK: {train_kappa:.4f}, Validation QWK: {val_kappa:.4f}\")\n",
    "        clear_output(wait=True)\n",
    "\n",
    "    print(f\"Mean Train QWK --> {np.mean(train_S):.4f}\")\n",
    "    print(f\"Mean Validation QWK ---> {np.mean(test_S):.4f}\")\n",
    "\n",
    "    KappaOPtimizer = minimize(evaluate_predictions,\n",
    "                          x0=[0.5, 1.5, 2.5], args=(convert(y), oof_non_rounded), \n",
    "                          method='Nelder-Mead')\n",
    "    oof_tuned = threshold_Rounder(oof_non_rounded, KappaOPtimizer.x)\n",
    "    tKappa = quadratic_weighted_kappa(convert(y), oof_tuned)\n",
    "    \n",
    "    assert KappaOPtimizer.success, \"Optimization did not converge.\"\n",
    "    \n",
    "    \n",
    "\n",
    "    print(f\"----> || Optimized QWK SCORE :: {Fore.CYAN}{Style.BRIGHT} {tKappa:.3f}{Style.RESET_ALL}\")\n",
    "\n",
    "    tpm = test_preds.mean(axis=1)\n",
    "    tpTuned = threshold_Rounder(tpm, KappaOPtimizer.x)\n",
    "    \n",
    "    submission = pd.DataFrame({\n",
    "        'id': sample['id'],\n",
    "        'sii': tpTuned\n",
    "    })\n",
    "\n",
    "    return submission\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "\n",
    "class HybridVotingRegressor(BaseEstimator, RegressorMixin):\n",
    "    def __init__(self, models, model_types):\n",
    "        \"\"\"\n",
    "        Hybrid Voting Regressor/Classifier\n",
    "        \n",
    "        :param models: List of (name, model) tuples.\n",
    "        :param model_types: List of strings, either 'regression' or 'classification',\n",
    "                            corresponding to the type of each model.\n",
    "        \"\"\"\n",
    "        self.models = models\n",
    "        self.model_types = model_types\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Fit all models in the ensemble.\n",
    "        \"\"\"\n",
    "        for model, model_type in zip(self.models, self.model_types):\n",
    "            if model_type == 'classification':\n",
    "                y_transformed = convert(y)  # Ensure y is discrete for classifiers\n",
    "            else:\n",
    "                y_transformed = y\n",
    "            model[1].fit(X, y_transformed)\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        classification_preds=[]\n",
    "        \"\"\"\n",
    "        Predict using all models and combine predictions.\n",
    "        For regression models, take the mean.\n",
    "        For classification models, take the mode.\n",
    "        \"\"\"\n",
    "        for model, model_type in zip(self.models, self.model_types):\n",
    "            preds = model[1].predict(X)\n",
    "            if model_type == 'classification':\n",
    "                classification_preds.append(np.round(preds))\n",
    "            else:\n",
    "                classification_preds.append(convert(preds))\n",
    "        \n",
    "        # Combine predictions\n",
    "        if classification_preds:\n",
    "            classification_preds = np.rint(np.mean(classification_preds, axis=0))\n",
    "        else:\n",
    "            classification_preds = 0  # No classification models\n",
    " \n",
    "        return classification_preds\n",
    "# Model parameters for LightGBM\n",
    "Params = {\n",
    "    'learning_rate': 0.046,\n",
    "    'max_depth': 12,\n",
    "    'num_leaves': 478,\n",
    "    'min_data_in_leaf': 13,\n",
    "    'feature_fraction': 0.893,\n",
    "    'bagging_fraction': 0.784,\n",
    "    'bagging_freq': 4,\n",
    "    'lambda_l1': 10,  # Increased from 6.59\n",
    "    'lambda_l2': 0.01,  # Increased from 2.68e-06\n",
    "    'device': 'gpu'\n",
    "\n",
    "}\n",
    "\n",
    "\n",
    "# XGBoost parameters\n",
    "XGB_Params = {\n",
    "    'max_depth': 3, 'n_estimators': 58, 'learning_rate': 0.07327652118259573, 'subsample': 0.5968194045365575, 'colsample_bytree': 0.9123669348125403,\n",
    "    'reg_alpha': 1,  # Increased from 0.1\n",
    "    'reg_lambda': 5,  # Increased from 1\n",
    "    'random_state': SEED,\n",
    "    'tree_method': 'gpu_hist',\n",
    "    \n",
    "}\n",
    "\n",
    "\n",
    "CatBoost_Params = {\n",
    "    'learning_rate': 0.05,\n",
    "    'depth': 6,\n",
    "    'iterations': 200,\n",
    "    'random_seed': SEED,\n",
    "    'verbose': 0,\n",
    "    'l2_leaf_reg': 10,  # Increase this value\n",
    "    'task_type': 'GPU'\n",
    "\n",
    "}\n",
    "\n",
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pytorch_tabnet.callbacks import Callback\n",
    "import os\n",
    "import torch\n",
    "from pytorch_tabnet.callbacks import Callback\n",
    "\n",
    "class TabNetWrapper(BaseEstimator, RegressorMixin):\n",
    "    def __init__(self, **kwargs):\n",
    "        self.model = TabNetRegressor(**kwargs)\n",
    "        self.kwargs = kwargs\n",
    "        self.imputer = SimpleImputer(strategy='median')\n",
    "        self.best_model_path = 'best_tabnet_model.pt'\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        # Handle missing values\n",
    "        X_imputed = self.imputer.fit_transform(X)\n",
    "        \n",
    "        if hasattr(y, 'values'):\n",
    "            y = y.values\n",
    "            \n",
    "        # Create internal validation set\n",
    "        X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "            X_imputed, \n",
    "            y, \n",
    "            test_size=0.2,\n",
    "            random_state=42\n",
    "        )\n",
    "        \n",
    "        # Train TabNet model\n",
    "        history = self.model.fit(\n",
    "            X_train=X_train,\n",
    "            y_train=y_train.reshape(-1, 1),\n",
    "            eval_set=[(X_valid, y_valid.reshape(-1, 1))],\n",
    "            eval_name=['valid'],\n",
    "            eval_metric=['mse'],\n",
    "            max_epochs=500,\n",
    "            patience=50,\n",
    "            batch_size=1024,\n",
    "            virtual_batch_size=128,\n",
    "            num_workers=0,\n",
    "            drop_last=False,\n",
    "            callbacks=[\n",
    "                TabNetPretrainedModelCheckpoint(\n",
    "                    filepath=self.best_model_path,\n",
    "                    monitor='valid_mse',\n",
    "                    mode='min',\n",
    "                    save_best_only=True,\n",
    "                    verbose=True\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        # Load the best model\n",
    "        if os.path.exists(self.best_model_path):\n",
    "            self.model.load_model(self.best_model_path)\n",
    "            os.remove(self.best_model_path)  # Remove temporary file\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        X_imputed = self.imputer.transform(X)\n",
    "        return self.model.predict(X_imputed).flatten()\n",
    "    \n",
    "    def __deepcopy__(self, memo):\n",
    "        # Add deepcopy support for scikit-learn\n",
    "        cls = self.__class__\n",
    "        result = cls.__new__(cls)\n",
    "        memo[id(self)] = result\n",
    "        for k, v in self.__dict__.items():\n",
    "            setattr(result, k, deepcopy(v, memo))\n",
    "        return result\n",
    "\n",
    "# TabNet hyperparameters\n",
    "TabNet_Params = {\n",
    "    'n_d': 64,              # Width of the decision prediction layer\n",
    "    'n_a': 64,              # Width of the attention embedding for each step\n",
    "    'n_steps': 5,           # Number of steps in the architecture\n",
    "    'gamma': 1.5,           # Coefficient for feature selection regularization\n",
    "    'n_independent': 2,     # Number of independent GLU layer in each GLU block\n",
    "    'n_shared': 2,          # Number of shared GLU layer in each GLU block\n",
    "    'lambda_sparse': 1e-4,  # Sparsity regularization\n",
    "    'optimizer_fn': torch.optim.Adam,\n",
    "    'optimizer_params': dict(lr=2e-2, weight_decay=1e-5),\n",
    "    'mask_type': 'entmax',\n",
    "    'scheduler_params': dict(mode=\"min\", patience=10, min_lr=1e-5, factor=0.5),\n",
    "    'scheduler_fn': torch.optim.lr_scheduler.ReduceLROnPlateau,\n",
    "    'verbose': 1,\n",
    "    'device_name': 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "}\n",
    "\n",
    "class TabNetPretrainedModelCheckpoint(Callback):\n",
    "    def __init__(self, filepath, monitor='val_loss', mode='min', \n",
    "                 save_best_only=True, verbose=1):\n",
    "        super().__init__()  # Initialize parent class\n",
    "        self.filepath = filepath\n",
    "        self.monitor = monitor\n",
    "        self.mode = mode\n",
    "        self.save_best_only = save_best_only\n",
    "        self.verbose = verbose\n",
    "        self.best = float('inf') if mode == 'min' else -float('inf')\n",
    "        \n",
    "    def on_train_begin(self, logs=None):\n",
    "        self.model = self.trainer  # Use trainer itself as model\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        logs = logs or {}\n",
    "        current = logs.get(self.monitor)\n",
    "        if current is None:\n",
    "            return\n",
    "        \n",
    "        # Check if current metric is better than best\n",
    "        if (self.mode == 'min' and current < self.best) or \\\n",
    "           (self.mode == 'max' and current > self.best):\n",
    "            if self.verbose:\n",
    "                print(f'\\nEpoch {epoch}: {self.monitor} improved from {self.best:.4f} to {current:.4f}')\n",
    "            self.best = current\n",
    "            if self.save_best_only:\n",
    "                self.model.save_model(self.filepath)  # Save the entire model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e9992e34",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-04T04:58:46.386593Z",
     "iopub.status.busy": "2024-12-04T04:58:46.386306Z",
     "iopub.status.idle": "2024-12-04T05:20:17.982695Z",
     "shell.execute_reply": "2024-12-04T05:20:17.981580Z"
    },
    "papermill": {
     "duration": 1291.602355,
     "end_time": "2024-12-04T05:20:17.985403",
     "exception": false,
     "start_time": "2024-12-04T04:58:46.383048",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Folds: 100%|██████████| 10/10 [21:31<00:00, 129.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Train QWK --> 0.6413\n",
      "Mean Validation QWK ---> 0.4242\n",
      "----> || Optimized QWK SCORE :: \u001b[36m\u001b[1m 0.424\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sii</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00008ff9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000fd460</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00105258</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00115b9f</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0016bb22</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>001f3379</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0038ba98</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0068a485</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0069fbed</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0083e397</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0087dd65</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>00abe655</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>00ae59c9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>00af6387</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>00bd4359</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>00c0cd71</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>00d56d4b</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>00d9913d</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>00e6167c</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>00ebc35d</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  sii\n",
       "0   00008ff9    1\n",
       "1   000fd460    0\n",
       "2   00105258    1\n",
       "3   00115b9f    1\n",
       "4   0016bb22    1\n",
       "5   001f3379    1\n",
       "6   0038ba98    1\n",
       "7   0068a485    0\n",
       "8   0069fbed    1\n",
       "9   0083e397    1\n",
       "10  0087dd65    1\n",
       "11  00abe655    1\n",
       "12  00ae59c9    1\n",
       "13  00af6387    1\n",
       "14  00bd4359    1\n",
       "15  00c0cd71    1\n",
       "16  00d56d4b    0\n",
       "17  00d9913d    0\n",
       "18  00e6167c    1\n",
       "19  00ebc35d    1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create model instances\n",
    "Light = LGBMRegressor(**Params, random_state=SEED, verbose=-1, n_estimators=300)\n",
    "XGB_Model = XGBRegressor(**XGB_Params)\n",
    "CatBoost_Model = CatBoostRegressor(**CatBoost_Params)\n",
    "TabNet_Model = TabNetWrapper(**TabNet_Params) # New\n",
    "train_xg_regressor = TrainXGMaskNNSRegressor(num_epochs=300, num_models=4, batch_size=64, lr=0.0025)\n",
    "\n",
    "# Define models\n",
    "models = [\n",
    "    ('lightgbm', Light),\n",
    "    ('xgboost', XGB_Model),\n",
    "    ('catboost', CatBoost_Model),\n",
    "    ('xg', train_xg_regressor),\n",
    "\n",
    "]\n",
    "model_types = ['classification', 'classification', 'classification', 'r']\n",
    "\n",
    "# Initialize the hybrid voting regressor\n",
    "hybrid_voter = HybridVotingRegressor(models=models, model_types=model_types)\n",
    "Submission=TrainML(hybrid_voter, test)\n",
    "Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9b3d46cd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-04T05:20:17.992909Z",
     "iopub.status.busy": "2024-12-04T05:20:17.992588Z",
     "iopub.status.idle": "2024-12-04T05:20:18.000811Z",
     "shell.execute_reply": "2024-12-04T05:20:18.000067Z"
    },
    "papermill": {
     "duration": 0.013809,
     "end_time": "2024-12-04T05:20:18.002446",
     "exception": false,
     "start_time": "2024-12-04T05:20:17.988637",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "Submission.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 9643020,
     "sourceId": 81933,
     "sourceType": "competition"
    },
    {
     "datasetId": 921302,
     "sourceId": 7453542,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30805,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1358.236671,
   "end_time": "2024-12-04T05:20:21.596872",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-12-04T04:57:43.360201",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
